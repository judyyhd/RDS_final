{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84895,"databundleVersionId":10008389,"sourceType":"competition"},{"sourceId":9616093,"sourceType":"datasetVersion","datasetId":5868381}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":8663.722737,"end_time":"2024-10-30T01:16:31.482057","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-29T22:52:07.75932","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and configs","metadata":{"papermill":{"duration":0.007257,"end_time":"2024-10-29T22:52:10.547302","exception":false,"start_time":"2024-10-29T22:52:10.540045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.base import clone\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom scipy.special import logit\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pickle\nimport shutil\nimport optuna\nimport json\nimport os\nimport gc\n\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":4.40862,"end_time":"2024-10-29T22:52:32.518494","exception":false,"start_time":"2024-10-29T22:52:28.109874","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    train_path = '/kaggle/input/playground-series-s4e11/train.csv'\n    test_path = '/kaggle/input/playground-series-s4e11/test.csv'\n    sample_sub_path = '/kaggle/input/playground-series-s4e11/sample_submission.csv'\n    \n    original_data_path = '/kaggle/input/depression-surveydataset-for-analysis/final_depression_dataset_1.csv'\n    \n    target = 'Depression'\n    n_folds = 5\n    seed = 42","metadata":{"papermill":{"duration":0.016779,"end_time":"2024-10-29T22:52:32.542135","exception":false,"start_time":"2024-10-29T22:52:32.525356","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data loading and preprocessing","metadata":{"papermill":{"duration":0.008326,"end_time":"2024-10-29T22:52:32.55769","exception":false,"start_time":"2024-10-29T22:52:32.549364","status":"completed"},"tags":[]}},{"cell_type":"code","source":"_train = pd.read_csv(CFG.train_path, index_col='id')\n_test = pd.read_csv(CFG.test_path, index_col='id')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Similar to previous months' playground competitions, the dataset this time is also imbalanced, with the majority consisting of negative cases for depression. The imbalance ratio is not very high, so we won't do any upsampling or downsampling.","metadata":{}},{"cell_type":"code","source":"depression_counts = _train.Depression.value_counts(normalize=True).reset_index()\ndepression_counts.columns = ['Depression', 'Proportion']\npalette = {0: sns.color_palette(\"Set1\")[1], 1: sns.color_palette(\"Set1\")[0]}\nclass_mapping = {0: 'Not Depressed', 1: 'Depressed'}\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Depression', y='Proportion', data=depression_counts, palette=palette)\n\nplt.title('')\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks([])\nplt.yticks([])\nplt.ylim(0, 1)\n\nfor index, row in depression_counts.iterrows():\n    plt.text(row.name, row.Proportion + 0.02, f'{class_mapping[row.name]} ({row.Proportion:.2%})', ha='center', va='bottom', fontsize=12)\n\nsns.despine()\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- There are several categorical columns in the dataset, and some contain missing values. We will leave the missing values as they are, as most of the models we'll be using can handle missing values on their own.\n- Some categorical columns have high cardinality, while others have only two unique values.","metadata":{}},{"cell_type":"code","source":"cat_cols = _train.select_dtypes(include='object').columns.tolist()\npalette = {0: sns.color_palette(\"Set1\")[1], 1: sns.color_palette(\"Set1\")[0]}\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 10))\naxes = axes.flatten()\nfor i, col in enumerate(cat_cols):\n    sns.countplot(data=_train, x=col, hue=CFG.target, ax=axes[i], palette=palette)\n    axes[i].set_title(f'{col} distribution')\n    axes[i].set_ylabel('')\n    axes[i].set_xlabel('')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 5))\nsns.barplot(\n    x=_train[cat_cols].nunique().sort_values(ascending=False).values,\n    y=_train[cat_cols].nunique().sort_values(ascending=False).index,\n    ax=ax,\n    color=\"grey\"\n)\nax.set_title('Unique values in categorical columns')\n\nfor i, value in enumerate(_train[cat_cols].nunique().sort_values(ascending=False).values):\n    ax.text(value, i, f'{value}', va='center')\n\nsns.despine()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_null_values = _train.drop(columns=CFG.target).isnull().T\ntest_null_values = _test.isnull().T\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\nsns.heatmap(train_null_values, cbar=False, ax=axes[0], cmap=sns.light_palette('grey', as_cmap=True))\naxes[0].set_title('Train')\naxes[0].set_xticks([]) \naxes[0].set_xlabel('')\n\nsns.heatmap(test_null_values, cbar=False, ax=axes[1], cmap=sns.light_palette('grey', as_cmap=True))\naxes[1].set_title('Test')\naxes[1].set_xticks([]) \naxes[1].set_xlabel('')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Below, we'll create a function that:\n  1. Converts the categorical columns to the `category` datatype.\n  2. Ensures that targets are consistent between the competition and original dataset.\n  3. Makes sure the data types are consistent between the original and competition datasets.\n  4. Divides both datasets into features and targets to be used by the `Trainer` class later on.","metadata":{}},{"cell_type":"code","source":"def get_data(model):\n    train = pd.read_csv(CFG.train_path, index_col='id')\n    test = pd.read_csv(CFG.test_path, index_col='id')\n    original = pd.read_csv(CFG.original_data_path)\n    original[CFG.target] = original[CFG.target].map({'Yes': 1, 'No': 0})\n    \n    if model == 'cb':\n        cat_cols = test.columns.tolist()\n    else:\n        cat_cols = test.select_dtypes(include='object').columns.tolist()\n        \n    train[cat_cols] = train[cat_cols].astype(str).astype('category')\n    test[cat_cols] = test[cat_cols].astype(str).astype('category')\n    original[cat_cols] = original[cat_cols].astype(str).astype('category')\n        \n    for col in cat_cols:\n        common_categories = train[col].cat.categories.union(original[col].cat.categories)\n        train[col] = train[col].cat.set_categories(common_categories)\n        original[col] = original[col].cat.set_categories(common_categories)\n    \n    for col in train.columns:\n        original[col] = original[col].astype(train[col].dtype)\n\n    if model in ['gb', 'adb']:\n        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        train[cat_cols] = encoder.fit_transform(train[cat_cols])\n        test[cat_cols] = encoder.transform(test[cat_cols])\n        original[cat_cols] = encoder.transform(original[cat_cols])\n    \n        cols_with_missing_values = train.columns[train.isnull().mean() > 0]\n        imputer = SimpleImputer(strategy='mean')\n        train[cols_with_missing_values] = imputer.fit_transform(train[cols_with_missing_values])\n        test[cols_with_missing_values] = imputer.transform(test[cols_with_missing_values])\n        original[cols_with_missing_values] = imputer.transform(original[cols_with_missing_values])\n    \n    X = train.drop(CFG.target, axis=1)\n    y = train[CFG.target]\n    X_test = test\n    \n    X_original = original.drop(CFG.target, axis=1)\n    y_original = original[CFG.target]\n    \n    return X, y, X_test, X_original, y_original","metadata":{"papermill":{"duration":0.022953,"end_time":"2024-10-29T22:52:34.217537","exception":false,"start_time":"2024-10-29T22:52:34.194584","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training base models","metadata":{"papermill":{"duration":0.010485,"end_time":"2024-10-29T22:52:38.731558","exception":false,"start_time":"2024-10-29T22:52:38.721073","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, config=CFG, is_ensemble_model=False):\n        self.model = model\n        self.config = config\n        self.is_ensemble_model = is_ensemble_model\n\n    def fit_predict(self, X, y, X_test, X_original=None, y_original=None, threshold=0.5):\n        print(f'Training {self.model.__class__.__name__}\\n')\n        \n        scores = []        \n        coeffs = np.zeros((1, X.shape[1]))\n        oof_pred_probs = np.zeros(X.shape[0])\n        test_pred_probs = np.zeros(X_test.shape[0])\n        \n        skf = StratifiedKFold(n_splits=self.config.n_folds, random_state=self.config.seed, shuffle=True)\n        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            if not self.is_ensemble_model:                                \n                X_train = pd.concat([X_train, X_original], ignore_index=True)\n                y_train = pd.concat([y_train, y_original], ignore_index=True)\n            \n            model = clone(self.model)\n            model.fit(X_train, y_train)\n            \n            if self.is_ensemble_model:\n                coeffs += model.coef_ / self.config.n_folds\n                if isinstance(self.model, LogisticRegression):\n                    n_iters = model.n_iter_[0]\n            \n            y_pred_probs = model.predict(X_val) if isinstance(self.model, Ridge) else model.predict_proba(X_val)[: ,1]\n            oof_pred_probs[val_idx] = y_pred_probs \n            \n            temp_test_pred_probs = model.predict(X_test) if isinstance(self.model, Ridge) else model.predict_proba(X_test)[:, 1]\n            test_pred_probs += temp_test_pred_probs / self.config.n_folds\n            \n            score = accuracy_score(y_val, (y_pred_probs > threshold).astype(int))\n            scores.append(score)\n            \n            del model, X_train, y_train, X_val, y_val, y_pred_probs\n            gc.collect()\n            \n            if self.is_ensemble_model and isinstance(self.model, LogisticRegression):\n                print(f'--- Fold {fold_idx + 1} - Accuracy: {score:.6f} ({n_iters} iterations)')\n            else:\n                print(f'--- Fold {fold_idx + 1} - Accuracy: {score:.6f}')\n            \n        overall_score = accuracy_score(y, (oof_pred_probs > threshold).astype(int))\n            \n        print(f'\\n------ Overall: {overall_score:.6f} | Average: {np.mean(scores):.6f} ± {np.std(scores):.6f}')\n        \n        if self.is_ensemble_model:\n            return oof_pred_probs, test_pred_probs, scores, coeffs\n        else:\n            os.makedirs('oof_pred_probs', exist_ok=True)\n            os.makedirs('test_pred_probs', exist_ok=True)\n            self._save_pred_probs(oof_pred_probs, np.mean(scores), 'oof')\n            self._save_pred_probs(test_pred_probs, np.mean(scores), 'test')\n            return oof_pred_probs, test_pred_probs, scores\n        \n    def fit(self, X, y, threshold=0.5):\n        scores = []                \n        skf = StratifiedKFold(n_splits=self.config.n_folds, random_state=self.config.seed, shuffle=True)\n        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            model = clone(self.model)\n            model.fit(X_train, y_train)\n            \n            y_pred_probs = model.predict(X_val) if isinstance(self.model, Ridge) else model.predict_proba(X_val)[:, 1]  \n            score = accuracy_score(y_val, (y_pred_probs > threshold).astype(int))\n            scores.append(score)\n            \n            del model, X_train, y_train, X_val, y_val, y_pred_probs\n            gc.collect() \n        \n        return np.mean(scores)\n        \n    def _save_pred_probs(self, pred_probs, cv_score, name):\n        model_name = self.model.__class__.__name__.lower().replace('classifier', '')\n        with open(f'{name}_pred_probs/{model_name}_{name}_pred_probs_{cv_score:.6f}.pkl', 'wb') as f:\n            pickle.dump(pred_probs, f)","metadata":{"papermill":{"duration":0.03433,"end_time":"2024-10-29T22:52:38.776658","exception":false,"start_time":"2024-10-29T22:52:38.742328","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_submission(name, test_pred_probs, score, threshold=0.5):\n    sub = pd.read_csv(CFG.sample_sub_path)\n    sub[CFG.target] = (test_pred_probs > threshold).astype(int)\n    sub.to_csv(f'sub_{name}_{score:.6f}.csv', index=False)\n    return sub.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_params = {\n    \"C\": 5.559884346567435,\n    \"max_iter\": 1000,\n    \"n_jobs\": 4,\n    \"penalty\": \"l2\",\n    \"random_state\": 42,\n    \"solver\": \"newton-cg\",\n    \"tol\": 0.08313081991676836\n}\n\ncb_params = {\n    \"border_count\": 180,\n    \"colsample_bylevel\": 0.7351678905666684,\n    \"depth\": 4,\n    \"iterations\": 2372,\n    \"l2_leaf_reg\": 4.442847441200204,\n    \"learning_rate\": 0.0514109059943355,\n    \"min_child_samples\": 146,\n    \"random_state\": 42,\n    \"random_strength\": 0.18678416655567043,\n    \"scale_pos_weight\": 1.019889465491297,\n    \"subsample\": 0.3511896501762123,\n    \"verbose\": False\n}\n\nxgb_params = {\n    \"colsample_bylevel\": 0.25155109886677396,\n    \"colsample_bynode\": 0.5723191165109757,\n    \"colsample_bytree\": 0.18034301813835885,\n    \"enable_categorical\": True,\n    \"gamma\": 3.6392698070258622,\n    \"max_bins\": 26161,\n    \"max_depth\": 16,\n    \"max_leaves\": 67,\n    \"min_child_weight\": 34,\n    \"n_estimators\": 3853,\n    \"n_jobs\": -1,\n    \"random_state\": 42,\n    \"reg_alpha\": 7.996080341061729,\n    \"reg_lambda\": 46.83054555763492,\n    \"scale_pos_weight\": 1.2157646356820928,\n    \"subsample\": 0.9117754083869292,\n    \"verbosity\": 0\n}\n\nlgbm_params = {\n    \"boosting_type\": \"gbdt\",\n    \"colsample_bytree\": 0.18283018243382332,\n    \"learning_rate\": 0.09945326391012832,\n    \"max_bins\": 36644,\n    \"min_child_samples\": 105,\n    \"min_child_weight\": 0.2083765599710974,\n    \"n_estimators\": 244,\n    \"n_jobs\": -1,\n    \"num_leaves\": 122,\n    \"random_state\": 42,\n    \"reg_alpha\": 8.662578235164972,\n    \"reg_lambda\": 3.5696291074963926,\n    \"scale_pos_weight\": 1.0733293968870794,\n    \"subsample\": 0.5360642841695424,\n    \"verbose\": -1\n}\n\nlgbm_goss_params = {\n    \"boosting_type\": \"goss\",\n    \"colsample_bytree\": 0.11905309670044416,\n    \"learning_rate\": 0.04641567005485582,\n    \"max_bins\": 18501,\n    \"min_child_samples\": 283,\n    \"min_child_weight\": 0.5242575557671028,\n    \"n_estimators\": 966,\n    \"n_jobs\": -1,\n    \"num_leaves\": 159,\n    \"random_state\": 42,\n    \"reg_alpha\": 8.160196501794122,\n    \"reg_lambda\": 9.861767101758469,\n    \"scale_pos_weight\": 0.991416082619142,\n    \"subsample\": 0.9259030065865966,\n    \"verbose\": -1\n}\n\nlgbm_dart_params = {\n    \"boosting_type\": \"dart\",\n    \"colsample_bytree\": 0.1181378019860333,\n    \"learning_rate\": 0.0910272033595692,\n    \"max_bins\": 7386,\n    \"min_child_samples\": 225,\n    \"min_child_weight\": 0.28492487885169293,\n    \"n_estimators\": 1454,\n    \"n_jobs\": -1,\n    \"num_leaves\": 163,\n    \"random_state\": 42,\n    \"reg_alpha\": 2.9686583338116925,\n    \"reg_lambda\": 9.781841345026509,\n    \"scale_pos_weight\": 0.8309734952725212,\n    \"subsample\": 0.511061530253864,\n    \"verbose\": -1\n}\n\ngb_params = {\n    'learning_rate': 0.129726178306296,\n    'max_depth': 82,\n    'max_features': 0.9815414831121304,\n    'max_leaf_nodes': 45,\n    'min_samples_leaf': 0.0015657248550179637,\n    'min_samples_split': 0.3385622260186072,\n    'min_weight_fraction_leaf': 0.03201568899384577,\n    'n_estimators': 1048,\n    'random_state': 42,\n    'subsample': 0.9550206349439808\n}\n\nadb_params = {\n    'learning_rate': 1.5174537517219886,\n    'n_estimators': 410,\n    'random_state': 42\n}","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.026657,"end_time":"2024-10-29T22:52:38.814143","exception":false,"start_time":"2024-10-29T22:52:38.787486","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = {}\noof_pred_probs = {}\ntest_pred_probs = {}","metadata":{"papermill":{"duration":0.018822,"end_time":"2024-10-29T22:52:38.843819","exception":false,"start_time":"2024-10-29T22:52:38.824997","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/competitions/playground-series-s4e11/discussion/543746\n\nX, y, X_test, X_original, y_original = get_data('lr')\nlr_model = make_pipeline(\n    OneHotEncoder(handle_unknown='ignore', min_frequency=0.001),\n    LogisticRegression(**lr_params)\n)\nlr_trainer = Trainer(lr_model)\noof_pred_probs['LogisticRegression'], test_pred_probs['LogisticRegression'], scores['LogisticRegression'] = lr_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"cb\")\ncb_model = CatBoostClassifier(**cb_params, cat_features=X.columns.tolist())\ncb_trainer = Trainer(cb_model)\noof_pred_probs['CatBoost'], test_pred_probs['CatBoost'], scores['CatBoost'] = cb_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"xgb\")\nxgb_model = XGBClassifier(**xgb_params)\nxgb_trainer = Trainer(xgb_model)\noof_pred_probs['XGBoost'], test_pred_probs['XGBoost'], scores['XGBoost'] = xgb_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"papermill":{"duration":37.273521,"end_time":"2024-10-29T23:26:05.942167","exception":false,"start_time":"2024-10-29T23:25:28.668646","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"lgbm\")\nlgbm_model = LGBMClassifier(**lgbm_params)\nlgbm_trainer = Trainer(lgbm_model)\noof_pred_probs['LightGBM'], test_pred_probs['LightGBM'], scores['LightGBM'] = lgbm_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"lgbm_goss\")\nlgbm_goss_model = LGBMClassifier(**lgbm_goss_params)\nlgbm_goss_trainer = Trainer(lgbm_goss_model)\noof_pred_probs['LightGBM (goss)'], test_pred_probs['LightGBM (goss)'], scores['LightGBM (goss)'] = lgbm_goss_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"lgbm_dart\")\nlgbm_dart_model = LGBMClassifier(**lgbm_dart_params)\nlgbm_dart_trainer = Trainer(lgbm_dart_model)\noof_pred_probs['LightGBM (dart)'], test_pred_probs['LightGBM (dart)'], scores['LightGBM (dart)'] = lgbm_dart_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"gb\")\ngb_model = GradientBoostingClassifier(**gb_params)\ngb_trainer = Trainer(gb_model)\noof_pred_probs['Gradient Boosting'], test_pred_probs['Gradient Boosting'], scores['Gradient Boosting'] = gb_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y, X_test, X_original, y_original = get_data(\"adb\")\nadb_model = AdaBoostClassifier(**adb_params)\nadb_trainer = Trainer(adb_model)\noof_pred_probs['AdaBoost'], test_pred_probs['AdaBoost'], scores['AdaBoost'] = adb_trainer.fit_predict(X, y, X_test, X_original, y_original)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# L2 Logistic Regression\n\nTo ensemble the OOF files of the base models, we will:\n1. Tune and train a `LogisticRegression` model.\n2. Tune and train a `Ridge` model.\n3. Combine the two ensembles using a weighted average.","metadata":{"papermill":{"duration":0.015703,"end_time":"2024-10-30T00:52:11.751773","exception":false,"start_time":"2024-10-30T00:52:11.73607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_weights(weights, title):\n    sorted_indices = np.argsort(weights[0])[::-1]\n    sorted_coeffs = np.array(weights[0])[sorted_indices]\n    sorted_model_names = np.array(list(oof_pred_probs.keys()))[sorted_indices]\n\n    plt.figure(figsize=(10, weights.shape[1] * 0.5))\n    ax = sns.barplot(x=sorted_coeffs, y=sorted_model_names, palette=\"RdYlGn_r\")\n\n    for i, (value, name) in enumerate(zip(sorted_coeffs, sorted_model_names)):\n        if value >= 0:\n            ax.text(value, i, f'{value:.3f}', va='center', ha='left', color='black')\n        else:\n            ax.text(value, i, f'{value:.3f}', va='center', ha='right', color='black')\n\n    xlim = ax.get_xlim()\n    ax.set_xlim(xlim[0] - 0.1 * abs(xlim[0]), xlim[1] + 0.1 * abs(xlim[1]))\n\n    plt.title(title)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l2_oof_pred_probs = {}\nl2_test_pred_probs = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = logit(pd.DataFrame(oof_pred_probs).clip(1e-15, 1-1e-15))\nX_test = logit(pd.DataFrame(test_pred_probs).clip(1e-15, 1-1e-15))","metadata":{"papermill":{"duration":0.046181,"end_time":"2024-10-30T00:52:11.813937","exception":false,"start_time":"2024-10-30T00:52:11.767756","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    solver_penalty_options = [\n        ('liblinear', 'l1'),\n        ('liblinear', 'l2'),\n        ('lbfgs', 'l2'),\n        ('lbfgs', None),\n        ('newton-cg', 'l2'),\n        ('newton-cg', None),\n        ('newton-cholesky', 'l2'),\n        ('newton-cholesky', None)\n    ]\n    solver, penalty = trial.suggest_categorical('solver_penalty', solver_penalty_options)\n    \n    params = {\n        'random_state': CFG.seed,\n        'max_iter': 500,\n        'C': trial.suggest_float('C', 0, 10),\n        'tol': trial.suggest_float('tol', 1e-10, 1e-1),\n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n        'solver': solver,\n        'penalty': penalty\n    }\n    \n    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.001)\n    \n    model = LogisticRegression(**params)\n    trainer = Trainer(model, is_ensemble_model=True)\n    return trainer.fit(X, y, threshold)\n\nsampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=50, n_jobs=-1)\nbest_params = study.best_params","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solver, penalty = best_params['solver_penalty']\nlr_params = {\n    'random_state': CFG.seed,\n    'max_iter': 500,\n    'C': best_params['C'],\n    'tol': best_params['tol'],\n    'fit_intercept': best_params['fit_intercept'],\n    'class_weight': best_params['class_weight'],\n    'solver': solver,\n    'penalty': penalty\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(json.dumps(lr_params, indent=2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_threshold = study.best_params['threshold']\nprint(f'Best threshold: {best_threshold:.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr_model = LogisticRegression(**lr_params)\nlr_trainer = Trainer(lr_model, is_ensemble_model=True)\nl2_oof_pred_probs['l2-ensemble-lr'], l2_test_pred_probs['l2-ensemble-lr'], scores['L2 Ensemble LR'], lr_coeffs = lr_trainer.fit_predict(X, y, X_test, None, None, best_threshold)","metadata":{"papermill":{"duration":3.260467,"end_time":"2024-10-30T01:16:28.837166","exception":false,"start_time":"2024-10-30T01:16:25.576699","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_submission('l2-ensemble-lr', l2_test_pred_probs['l2-ensemble-lr'], np.mean(scores['L2 Ensemble LR']), best_threshold)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_weights(lr_coeffs, 'LR Coefficients')","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.451627,"end_time":"2024-10-30T01:16:29.437399","exception":false,"start_time":"2024-10-30T01:16:28.985772","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# L2 Ridge","metadata":{}},{"cell_type":"code","source":"X = pd.DataFrame(oof_pred_probs)\nX_test = pd.DataFrame(test_pred_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):    \n    params = {\n        'random_state': CFG.seed,\n        'alpha': trial.suggest_float('alpha', 0, 50),\n        'tol': trial.suggest_float('tol', 1e-10, 1e-2),\n        'positive': trial.suggest_categorical('positive', [True, False]),\n        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False])\n    }\n    \n    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.001)\n    \n    model = Ridge(**params)\n    trainer = Trainer(model, is_ensemble_model=True)\n    return trainer.fit(X, y, threshold)\n\n\nsampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=50, n_jobs=-1)\nbest_params = study.best_params","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ridge_params = {\n    'random_state': CFG.seed,\n    'alpha': best_params['alpha'],\n    'tol': best_params['tol'],\n    'positive': best_params['positive'],\n    'fit_intercept': best_params['fit_intercept']\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(json.dumps(ridge_params, indent=2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_threshold = study.best_params['threshold']\nprint(f'Best threshold: {best_threshold:.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ridge_model = Ridge(**ridge_params)\nridge_trainer = Trainer(ridge_model, is_ensemble_model=True)\nl2_oof_pred_probs['l2-ensemble-ridge'], l2_test_pred_probs['l2-ensemble-ridge'], scores['L2 Ensemble Ridge'], ridge_coeffs = ridge_trainer.fit_predict(X, y, X_test, None, None, best_threshold)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_submission('l2-ensemble-ridge', l2_test_pred_probs['l2-ensemble-ridge'], np.mean(scores['L2 Ensemble Ridge']), best_threshold)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_weights(ridge_coeffs, 'Ridge Coefficients')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# L3 Weighted ensemble","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    weights = np.array([trial.suggest_float(l2_model_name, 0, 1) for l2_model_name in l2_oof_pred_probs.keys()])\n    weights /= np.sum(weights)\n    \n    preds = np.zeros(len(y))\n    for model, weight in zip(l2_oof_pred_probs.keys(), weights):\n        preds += l2_oof_pred_probs[model] * weight\n        \n    threshold = trial.suggest_float('threshold', 0.4, 0.6, step=0.001)\n            \n    return accuracy_score(y, (preds > threshold).astype(int))\n\nsampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=100, n_jobs=-1)","metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores['L3 Weighted Ensemble (LR + Ridge)'] = [study.best_value] * CFG.n_folds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_weights = np.array([study.best_params[l2_model] for l2_model in l2_oof_pred_probs.keys()])\nbest_weights /= np.sum(best_weights)\nprint(json.dumps({model: weight for model, weight in zip(l2_oof_pred_probs.keys(), best_weights)}, indent=2))\n\nbest_threshold = study.best_params['threshold']\nprint(f'\\nBest threshold: {best_threshold:.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nplt.pie(\n    [v for k,v in study.best_params.items() if k != \"threshold\"], \n    labels=[k for k,v in study.best_params.items() if k != \"threshold\"], \n    autopct='%1.1f%%',\n    colors=sns.color_palette('Set2', 2)\n)\nplt.title(\"L3 Ensemble Weights\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weighted_test_preds = np.zeros((X_test.shape[0]))\nfor model, weight in zip(l2_test_pred_probs.keys(), best_weights):\n    weighted_test_preds += l2_test_pred_probs[model] * weight\n    \nsave_submission('l3-weighted-ensemble', weighted_test_preds, np.mean(scores['L3 Weighted Ensemble (LR + Ridge)']), best_threshold)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results","metadata":{"papermill":{"duration":0.049579,"end_time":"2024-10-30T01:16:29.537891","exception":false,"start_time":"2024-10-30T01:16:29.488312","status":"completed"},"tags":[]}},{"cell_type":"code","source":"scores = pd.DataFrame(scores)\nmean_scores = scores.mean().sort_values(ascending=False)\norder = scores.mean().sort_values(ascending=False).index.tolist()\n\nmin_score = mean_scores.min()\nmax_score = mean_scores.max()\npadding = (max_score - min_score) * 0.5\nlower_limit = min_score - padding\nupper_limit = max_score + padding\n\nfig, axs = plt.subplots(1, 2, figsize=(15, scores.shape[1] * 0.3))\n\nboxplot = sns.boxplot(data=scores, order=order, ax=axs[0], orient='h', color='grey')\naxs[0].set_title('Fold Accuracy')\naxs[0].set_xlabel('')\naxs[0].set_ylabel('')\n\nbarplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], color='grey')\naxs[1].set_title('Average Accuracy')\naxs[1].set_xlabel('')\naxs[1].set_xlim(left=lower_limit, right=upper_limit)\naxs[1].set_ylabel('')\n\nfor i, (score, model) in enumerate(zip(mean_scores.values, mean_scores.index)):\n    color = 'cyan' if 'ensemble' in model.lower() else 'grey'\n    barplot.patches[i].set_facecolor(color)\n    boxplot.patches[i].set_facecolor(color)\n    barplot.text(score, i, round(score, 6), va='center')\n\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":false,"papermill":{"duration":0.810991,"end_time":"2024-10-30T01:16:30.398622","exception":false,"start_time":"2024-10-30T01:16:29.587631","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.rmtree('catboost_info', ignore_errors=True)","metadata":{"papermill":{"duration":0.060733,"end_time":"2024-10-30T01:16:30.509571","exception":false,"start_time":"2024-10-30T01:16:30.448838","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}